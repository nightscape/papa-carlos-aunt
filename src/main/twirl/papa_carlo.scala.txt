@(tree: Seq[SLexerRule])

@lexerToken(lexerAltList: LexerAltListContext) = @{
  val alternatives = lexerAltList.lexerAlt
  val content = ""//alternatives.getText
  val correctlyQuoted = content.replaceAll("'", "\"")
  s"chunk($correctlyQuoted)"
}

@single2DoubleQuotes(s: String) = @{
  s.replaceAll("'", "\"")
}



import name.lakhin.eliah.projects.papacarlo.{Syntax, Lexer}
import name.lakhin.eliah.projects.papacarlo.lexis.{Contextualizer, Matcher,  Tokenizer}
import name.lakhin.eliah.projects.papacarlo.syntax.{Expressions, Rule}

object Calculator {
  private def tokenizer = {
    val tokenizer = new Tokenizer()

    import tokenizer._
    import Matcher._


	@for(lexerRule <- tree) {
    tokenCategory(
      "@lexerRule.name",
      choice(
      @for(alt <- lexerRule.alternatives) {
        
      	sequence(@alt.mkString(",")"))
      }
    ).skip
	}

    tokenCategory(
      "number",
      choice(
        chunk("0"),
        sequence("\\", "u", optional("u"),"HexDigit", "HexDigit", "HexDigit", "HexDigit"))
        //sequence(rangeOf('1', '9'), zeroOrMore(rangeOf('0', '9')))
      )
    )
    

    terminals("(", ")", "%", "+", "-", "*", "/")

    tokenizer
  }

  def lexer = new Lexer(tokenizer, new Contextualizer)

  def syntax(lexer: Lexer) = new {
    val syntax = new Syntax(lexer)

    import syntax._
    import Rule._
    import Expressions._

    rule("expression").main {
      val rule =
        expression(branch("operand", recover(number, "operand required")))

      group(rule, "(", ")")
      postfix(rule, "%", 1)
      prefix(rule, "+", 2)
      prefix(rule, "-", 2)
      infix(rule, "*", 3)
      infix(rule, "/", 3, rightAssociativity = true)
      infix(rule, "+", 4)
      infix(rule, "-", 4)

      rule
    }

    val number = rule("number") {capture("value", token("number"))}
  }.syntax
}